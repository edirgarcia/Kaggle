{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General information\n",
    "\n",
    "This kernel is created using data of `Predicting Molecular Properties` competition.\n",
    "We have information about atom couples in molecules and need to predict `scalar_coupling_constant` between these atoms.\n",
    "\n",
    "![](http://www.et.byu.edu/~rowley/VLEfinal/methane_dimer.gif)\n",
    "\n",
    "In this kernel I'll do EDA and will try some approaches to modelling.\n",
    "\n",
    "*Work still in progress*\n",
    "\n",
    "\n",
    "~Thanks to the new kaggle update we can write code in kernels and import it. This is much more convenient and useful. I'm moving all the functions I can into this script: https://www.kaggle.com/artgor/artgor-utils So if you see somewhere code like artgot_utils.function_name(parameters) - it is from this script~\n",
    "I have realized that using utility scripts isn't very convenient, so I move all the code from that script into the notebook. But "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '/opt/caffe/python',\n",
       " '/opt/caffe2/build',\n",
       " '/data/home/edirga/notebooks/repos/kaggle/champs-scalar-coupling',\n",
       " '/anaconda/envs/py35/lib/python35.zip',\n",
       " '/anaconda/envs/py35/lib/python3.5',\n",
       " '/anaconda/envs/py35/lib/python3.5/plat-linux',\n",
       " '/anaconda/envs/py35/lib/python3.5/lib-dynload',\n",
       " '/anaconda/envs/py35/lib/python3.5/site-packages',\n",
       " '/anaconda/envs/py35/lib/python3.5/site-packages/dlib-19.15.0-py3.5-linux-x86_64.egg',\n",
       " '/anaconda/envs/py35/lib/python3.5/site-packages/xgboost-0.80-py3.5.egg',\n",
       " '/anaconda/envs/py35/lib/python3.5/site-packages/mxnet-1.3.0-py3.5.egg',\n",
       " '/anaconda/envs/py35/lib/python3.5/site-packages/graphviz-0.8.4-py3.5.egg',\n",
       " '/anaconda/envs/py35/lib/python3.5/site-packages/requests-2.18.4-py3.5.egg',\n",
       " '/anaconda/envs/py35/lib/python3.5/site-packages/tf2onnx-0.4.0-py3.5.egg',\n",
       " '/data/anaconda/envs/py35/lib/python3.5/site-packages',\n",
       " '/anaconda/envs/py35/lib/python3.5/site-packages/IPython/extensions',\n",
       " '/data/home/edirga/.ipython']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "current_path = !pwd\n",
    "current_path[0]\n",
    "\n",
    "if not current_path[0] in sys.path:\n",
    "    sys.path.insert(0, current_path[0])\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata: done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.6.3\n",
      "  latest version: 4.6.14\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /anaconda/envs/py35\n",
      "\n",
      "  added / updated specs:\n",
      "    - altair\n",
      "    - notebook\n",
      "    - vega\n",
      "    - vega_datasets\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    altair-2.2.2               |           py35_1         462 KB  conda-forge\n",
      "    ca-certificates-2019.3.9   |       hecc5488_0         146 KB  conda-forge\n",
      "    certifi-2018.8.24          |        py35_1001         139 KB  conda-forge\n",
      "    notebook-5.7.0             |           py35_0         7.3 MB  conda-forge\n",
      "    openssl-1.0.2r             |       h14c3975_0         3.1 MB  conda-forge\n",
      "    prometheus_client-0.7.0    |             py_0          38 KB  conda-forge\n",
      "    vega-1.4.0                 |           py35_1         1.6 MB  conda-forge\n",
      "    vega_datasets-0.7.0        |             py_0         176 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        13.0 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  altair             conda-forge/linux-64::altair-2.2.2-py35_1\n",
      "  prometheus_client  conda-forge/noarch::prometheus_client-0.7.0-py_0\n",
      "  vega               conda-forge/linux-64::vega-1.4.0-py35_1\n",
      "  vega_datasets      conda-forge/noarch::vega_datasets-0.7.0-py_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    pkgs/main::ca-certificates-2019.1.23-0 --> conda-forge::ca-certificates-2019.3.9-hecc5488_0\n",
      "  certifi               pkgs/main::certifi-2018.8.24-py35_1 --> conda-forge::certifi-2018.8.24-py35_1001\n",
      "  notebook                 pkgs/main::notebook-5.5.0-py35_0 --> conda-forge::notebook-5.7.0-py35_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  openssl              pkgs/main::openssl-1.0.2r-h7b6447c_0 --> conda-forge::openssl-1.0.2r-h14c3975_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "ca-certificates-2019 | 146 KB    | ##################################### | 100% \n",
      "altair-2.2.2         | 462 KB    | ##################################### | 100% \n",
      "certifi-2018.8.24    | 139 KB    | ##################################### | 100% \n",
      "vega-1.4.0           | 1.6 MB    | ##################################### | 100% \n",
      "vega_datasets-0.7.0  | 176 KB    | ##################################### | 100% \n",
      "notebook-5.7.0       | 7.3 MB    | ##################################### | 100% \n",
      "openssl-1.0.2r       | 3.1 MB    | ##################################### | 100% \n",
      "prometheus_client-0. | 38 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!conda install --yes --prefix {sys.prefix} -c conda-forge altair vega_datasets notebook vega"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (artgor_utils.py, line 58)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/anaconda/envs/py35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2963\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-31cedaf65eff>\"\u001b[0;36m, line \u001b[0;32m36\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    import artgor_utils\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/data/home/edirga/notebooks/repos/kaggle/champs-scalar-coupling/artgor_utils.py\"\u001b[0;36m, line \u001b[0;32m58\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m       \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import NuSVR, SVR\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "pd.options.display.precision = 15\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import datetime\n",
    "#from catboost import CatBoostRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.display import HTML\n",
    "import json\n",
    "import altair as alt\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "alt.renderers.enable('notebook')\n",
    "import artgor_utils\n",
    "\n",
    "# setting up altair\n",
    "workaround = artgor_utils.prepare_altair()\n",
    "HTML(\"\".join((\n",
    "    \"<script>\",\n",
    "    workaround,\n",
    "    \"</script>\",\n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a lot of files, let's focus on the main ones for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "sub = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {train.shape[0]} rows in train data.')\n",
    "print(f'There are {test.shape[0]} rows in test data.')\n",
    "\n",
    "print(f\"There are {train['molecule_name'].nunique()} distinct molecules in train data.\")\n",
    "print(f\"There are {test['molecule_name'].nunique()} distinct molecules in test data\")\n",
    "print(f\"There are {train['atom_index_0'].nunique()} unique atoms:\")\n",
    "print(train['atom_index_0'].unique())\n",
    "print(f\"There are {train['type'].nunique()} unique types.\")\n",
    "print(train['type'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in out main data files we have information about moleculas and pairs of atoms\n",
    "- test set in ~2 times smaller that train set;\n",
    "- we have 28 unique atoms types and 8 coupling types;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "atom_count = train['atom_index_0'].value_counts().reset_index().rename(columns={'atom_index_0': 'count', 'index': 'atom_index_0'})\n",
    "chart1 = alt.Chart(atom_count).mark_bar().encode(\n",
    "    x=alt.X(\"atom_index_0:N\", axis=alt.Axis(title='atom_index_0')),\n",
    "    y=alt.Y('count:Q', axis=alt.Axis(title='Count')),\n",
    "    tooltip=['atom_index_0', 'count']\n",
    ").properties(title=\"Counts of atom_index_0\", width=350).interactive()\n",
    "\n",
    "atom_count = train['atom_index_1'].value_counts().reset_index().rename(columns={'atom_index_1': 'count', 'index': 'atom_index_1'})\n",
    "chart2 = alt.Chart(atom_count).mark_bar().encode(\n",
    "    x=alt.X(\"atom_index_1:N\", axis=alt.Axis(title='atom_index_1')),\n",
    "    y=alt.Y('count:Q', axis=alt.Axis(title='Count')),\n",
    "    tooltip=['atom_index_1', 'count']\n",
    ").properties(title=\"Counts of atom_index_1\", width=350).interactive()\n",
    "\n",
    "type_count = train['type'].value_counts().reset_index().rename(columns={'type': 'count', 'index': 'type'})\n",
    "chart3 = alt.Chart(type_count).mark_bar().encode(\n",
    "    x=alt.X(\"type:N\", axis=alt.Axis(title='type')),\n",
    "    y=alt.Y('count:Q', axis=alt.Axis(title='Count')),\n",
    "    tooltip=['type', 'count']\n",
    ").properties(title=\"Counts of type\", width=350).interactive()\n",
    "\n",
    "hist_df = pd.cut(train['scalar_coupling_constant'], 20).value_counts().sort_index().reset_index().rename(columns={'index': 'bins'})\n",
    "hist_df['bins'] = hist_df['bins'].astype(str)\n",
    "chart4 = alt.Chart(hist_df).mark_bar().encode(\n",
    "    x=alt.X(\"bins:O\", axis=alt.Axis(title='Target bins')),\n",
    "    y=alt.Y('scalar_coupling_constant:Q', axis=alt.Axis(title='Count')),\n",
    "    tooltip=['scalar_coupling_constant', 'bins']\n",
    ").properties(title=\"scalar_coupling_constant histogram\", width=400).interactive()\n",
    "\n",
    "\n",
    "(chart1 | chart2) & (chart3 | chart4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (18, 6))\n",
    "plt.subplot(1, 2, 1);\n",
    "plt.hist(train['scalar_coupling_constant'], bins=20);\n",
    "plt.title('Basic scalar_coupling_constant histogram');\n",
    "plt.subplot(1, 2, 2);\n",
    "sns.violinplot(x='type', y='scalar_coupling_constant', data=train);\n",
    "plt.title('Violinplot of scalar_coupling_constant by type');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many interesting things here:\n",
    "- among first atoms there is a little number of atoms with index lower than 7 or higher than 24;\n",
    "- among second atoms there is a little number of atoms with index higher than 24. Also index with atom with index 9 in quite rare;\n",
    "- coupling types are unevenly distributed. There are 3 very popular, 3 quite rare and 2 with medium frequency;\n",
    "- target variable has a bimodal distribution;\n",
    "- different coupling types have really different values of target variable. Maybe it would make sense to build separate models for each of them;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting network graphs by type\n",
    "\n",
    "We have molecules, atom pairs, so this means data, which is interconnected. Network graphs should be useful to visualize such data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (20, 12))\n",
    "for i, t in enumerate(train['type'].unique()):\n",
    "    train_type = train.loc[train['type'] == t]\n",
    "    G = nx.from_pandas_edgelist(train_type, 'atom_index_0', 'atom_index_1', ['scalar_coupling_constant'])\n",
    "    plt.subplot(2, 4, i + 1);\n",
    "    nx.draw(G, with_labels=True);\n",
    "    plt.title(f'Graph for type {t}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! We can see that atom connections have different shapes for different types. Type 2JHH has an expecially unique scheme.\n",
    "Also we can see that some atoms are connected only to several other atoms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better network graphs\n",
    "But there is a little problem: as we saw earlier, there are atoms which are very rare, as a result graphs will be skewed due to them. Now I'll drop atoms for each type which are present in less then 1% of connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (20, 12))\n",
    "for i, t in enumerate(train['type'].unique()):\n",
    "    train_type = train.loc[train['type'] == t]\n",
    "    bad_atoms_0 = list(train_type['atom_index_0'].value_counts(normalize=True)[train_type['atom_index_0'].value_counts(normalize=True) < 0.01].index)\n",
    "    bad_atoms_1 = list(train_type['atom_index_1'].value_counts(normalize=True)[train_type['atom_index_1'].value_counts(normalize=True) < 0.01].index)\n",
    "    bad_atoms = list(set(bad_atoms_0 + bad_atoms_1))\n",
    "    train_type = train_type.loc[(train_type['atom_index_0'].isin(bad_atoms_0) == False) & (train_type['atom_index_1'].isin(bad_atoms_0) == False)]\n",
    "    G = nx.from_pandas_edgelist(train_type, 'atom_index_0', 'atom_index_1', ['scalar_coupling_constant'])\n",
    "    plt.subplot(2, 4, i + 1);\n",
    "    nx.draw(G, with_labels=True);\n",
    "    plt.title(f'Graph for type {t}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the graphs are much more clear!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "For now I'll use a basic approach to feature engineering.\n",
    "\n",
    "I'll use this useful kernel:\n",
    "https://www.kaggle.com/seriousran/just-speed-up-calculate-distance-from-benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structures = pd.read_csv('data/structures.csv')\n",
    "\n",
    "def map_atom_info(df, atom_idx):\n",
    "    df = pd.merge(df, structures, how = 'left',\n",
    "                  left_on  = ['molecule_name', f'atom_index_{atom_idx}'],\n",
    "                  right_on = ['molecule_name',  'atom_index'])\n",
    "    \n",
    "    df = df.drop('atom_index', axis=1)\n",
    "    df = df.rename(columns={'atom': f'atom_{atom_idx}',\n",
    "                            'x': f'x_{atom_idx}',\n",
    "                            'y': f'y_{atom_idx}',\n",
    "                            'z': f'z_{atom_idx}'})\n",
    "    return df\n",
    "\n",
    "train = map_atom_info(train, 0)\n",
    "train = map_atom_info(train, 1)\n",
    "\n",
    "test = map_atom_info(test, 0)\n",
    "test = map_atom_info(test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_p_0 = train[['x_0', 'y_0', 'z_0']].values\n",
    "train_p_1 = train[['x_1', 'y_1', 'z_1']].values\n",
    "test_p_0 = test[['x_0', 'y_0', 'z_0']].values\n",
    "test_p_1 = test[['x_1', 'y_1', 'z_1']].values\n",
    "\n",
    "train['dist'] = np.linalg.norm(train_p_0 - train_p_1, axis=1)\n",
    "test['dist'] = np.linalg.norm(test_p_0 - test_p_1, axis=1)\n",
    "train['dist_x'] = (train['x_0'] - train['x_1']) ** 2\n",
    "test['dist_x'] = (test['x_0'] - test['x_1']) ** 2\n",
    "train['dist_y'] = (train['y_0'] - train['y_1']) ** 2\n",
    "test['dist_y'] = (test['y_0'] - test['y_1']) ** 2\n",
    "train['dist_z'] = (train['z_0'] - train['z_1']) ** 2\n",
    "test['dist_z'] = (test['z_0'] - test['z_1']) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to create two new features: one will show the first character of the `type`, the second one will show other characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['type_0'] = train['type'].apply(lambda x: x[0])\n",
    "test['type_0'] = test['type'].apply(lambda x: x[0])\n",
    "train['type_1'] = train['type'].apply(lambda x: x[1:])\n",
    "test['type_1'] = test['type'].apply(lambda x: x[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, everyone uses this distance feature, let's have a look at it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (18, 8))\n",
    "plt.subplot(1, 2, 1);\n",
    "plt.hist(train['dist'], bins=20);\n",
    "plt.title('Basic dist_speedup histogram');\n",
    "plt.subplot(1, 2, 2);\n",
    "sns.violinplot(x='type', y='dist', data=train);\n",
    "plt.title('Violinplot of dist_speedup by type');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the values are quite different for different types!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['dist_to_type_mean'] = train['dist'] / train.groupby('type')['dist'].transform('mean')\n",
    "test['dist_to_type_mean'] = test['dist'] / test.groupby('type')['dist'].transform('mean')\n",
    "\n",
    "train['dist_to_type_0_mean'] = train['dist'] / train.groupby('type_0')['dist'].transform('mean')\n",
    "test['dist_to_type_0_mean'] = test['dist'] / test.groupby('type_0')['dist'].transform('mean')\n",
    "\n",
    "train['dist_to_type_1_mean'] = train['dist'] / train.groupby('type_1')['dist'].transform('mean')\n",
    "test['dist_to_type_1_mean'] = test['dist'] / test.groupby('type_1')['dist'].transform('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic model\n",
    "\n",
    "I'll use the function for metric calculation from this kernel: https://www.kaggle.com/abhishek/competition-metric\n",
    "\n",
    "UPD: use faster metric from this kernel: https://www.kaggle.com/uberkinder/efficient-metric\n",
    "\n",
    "You can see the code in my utility script. Please note that to use this metric for calculation you need to pass value `group_mae` to parameter `eval_metric`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.concat([train, pd.get_dummies(train['type'])], axis=1)\n",
    "# test = pd.concat([test, pd.get_dummies(test['type'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in ['atom_0', 'atom_1', 'type_0', 'type_1', 'type']:\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(train[f].values) + list(test[f].values))\n",
    "    train[f] = lbl.transform(list(train[f].values))\n",
    "    test[f] = lbl.transform(list(test[f].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(['id', 'molecule_name', 'scalar_coupling_constant'], axis=1)\n",
    "y = train['scalar_coupling_constant']\n",
    "X_test = test.drop(['id', 'molecule_name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'num_leaves': 128,\n",
    "          'min_child_samples': 79,\n",
    "          'objective': 'regression',\n",
    "          'max_depth': 9,\n",
    "          'learning_rate': 0.2,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"subsample_freq\": 1,\n",
    "          \"subsample\": 0.9,\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'mae',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.1,\n",
    "          'reg_lambda': 0.3,\n",
    "          'colsample_bytree': 1.0\n",
    "         }\n",
    "result_dict_lgb = artgor_utils.train_model_regression(X=X, X_test=X_test, y=y, params=params, folds=folds, model_type='lgb', eval_metric='group_mae', plot_feature_importance=True,\n",
    "                                                      verbose=1000, early_stopping_rounds=200, n_estimators=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['scalar_coupling_constant'] = result_dict_lgb['prediction']\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(result_dict_lgb['prediction'], bins=40);\n",
    "plt.title('Distribution of predictions');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot oof predictions vs target\n",
    "\n",
    "I use the code from benchmark kernel: https://www.kaggle.com/inversion/atomic-distance-benchmark\n",
    "\n",
    "Notice that while using `LabelEncoder` on categorical variables, `type` was the last feature, so `lbl` contains encodings for it and we can inverse_transform `type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = pd.DataFrame(y)\n",
    "plot_data.index.name = 'id'\n",
    "plot_data['yhat'] = result_dict_lgb['oof']\n",
    "plot_data['type'] = lbl.inverse_transform(X['type'])\n",
    "\n",
    "def plot_oof_preds(ctype, llim, ulim):\n",
    "        plt.figure(figsize=(6,6))\n",
    "        sns.scatterplot(x='scalar_coupling_constant',y='yhat',\n",
    "                        data=plot_data.loc[plot_data['type']==ctype,\n",
    "                        ['scalar_coupling_constant', 'yhat']]);\n",
    "        plt.xlim((llim, ulim))\n",
    "        plt.ylim((llim, ulim))\n",
    "        plt.plot([llim, ulim], [llim, ulim])\n",
    "        plt.xlabel('scalar_coupling_constant')\n",
    "        plt.ylabel('predicted')\n",
    "        plt.title(f'{ctype}', fontsize=18)\n",
    "        plt.show()\n",
    "\n",
    "plot_oof_preds('1JHC', 0, 250)\n",
    "plot_oof_preds('1JHN', 0, 100)\n",
    "plot_oof_preds('2JHC', -50, 50)\n",
    "plot_oof_preds('2JHH', -50, 50)\n",
    "plot_oof_preds('2JHN', -25, 25)\n",
    "plot_oof_preds('3JHC', -25, 100)\n",
    "plot_oof_preds('3JHH', -20, 20)\n",
    "plot_oof_preds('3JHN', -15, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that some types has better predictions than others. Maybe we should focus on improving models for these types?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
