{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "sub = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structures = pd.read_csv('data/structures.csv')\n",
    "\n",
    "def map_atom_info(df, atom_idx):\n",
    "    atom_idx = str(atom_idx)\n",
    "    df = pd.merge(df, structures, how = 'left',\n",
    "                  left_on  = ['molecule_name', 'atom_index_' + atom_idx ],\n",
    "                  right_on = ['molecule_name',  'atom_index'])\n",
    "    \n",
    "    df = df.drop('atom_index', axis=1)\n",
    "    df = df.rename(columns={'atom': 'atom_' + atom_idx ,\n",
    "                            'x': 'x_'+atom_idx,\n",
    "                            'y': 'y_'+atom_idx,\n",
    "                            'z': 'z_'+atom_idx})\n",
    "    return df\n",
    "\n",
    "train = map_atom_info(train, 0)\n",
    "train = map_atom_info(train, 1)\n",
    "\n",
    "test = map_atom_info(test, 0)\n",
    "test = map_atom_info(test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_p_0 = train[['x_0', 'y_0', 'z_0']].values\n",
    "train_p_1 = train[['x_1', 'y_1', 'z_1']].values\n",
    "test_p_0 = test[['x_0', 'y_0', 'z_0']].values\n",
    "test_p_1 = test[['x_1', 'y_1', 'z_1']].values\n",
    "\n",
    "train['dist'] = np.linalg.norm(train_p_0 - train_p_1, axis=1)\n",
    "test['dist'] = np.linalg.norm(test_p_0 - test_p_1, axis=1)\n",
    "train['dist_x'] = (train['x_0'] - train['x_1']) ** 2\n",
    "test['dist_x'] = (test['x_0'] - test['x_1']) ** 2\n",
    "train['dist_y'] = (train['y_0'] - train['y_1']) ** 2\n",
    "test['dist_y'] = (test['y_0'] - test['y_1']) ** 2\n",
    "train['dist_z'] = (train['z_0'] - train['z_1']) ** 2\n",
    "test['dist_z'] = (test['z_0'] - test['z_1']) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['dist_to_type_mean'] = train['dist'] / train.groupby('type')['dist'].transform('mean')\n",
    "test['dist_to_type_mean'] = test['dist'] / test.groupby('type')['dist'].transform('mean')\n",
    "\n",
    "train['dist_to_type_0_mean'] = train['dist'] / train.groupby('atom_0')['dist'].transform('mean')\n",
    "test['dist_to_type_0_mean'] = test['dist'] / test.groupby('atom_0')['dist'].transform('mean')\n",
    "\n",
    "train['dist_to_type_1_mean'] = train['dist'] / train.groupby('atom_1')['dist'].transform('mean')\n",
    "test['dist_to_type_1_mean'] = test['dist'] / test.groupby('atom_1')['dist'].transform('mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['type_0'] = train['type'].apply(lambda x: x[0])\n",
    "test['type_0'] = test['type'].apply(lambda x: x[0])\n",
    "train = train.drop('type', axis=1)\n",
    "test = test.drop('type', axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Neural Network implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_atom_0_series = train['atom_0']\n",
    "train_atom_0_dummies = pd.get_dummies(train_atom_0_series, prefix=\"atom_0\")\n",
    "test_atom_0_series = test['atom_0']\n",
    "test_atom_0_dummies = pd.get_dummies(test_atom_0_series, prefix=\"atom_0\")\n",
    "\n",
    "train_atom_1_series = train['atom_1']\n",
    "train_atom_1_dummies = pd.get_dummies(train_atom_1_series, prefix=\"atom_1\")\n",
    "test_atom_1_series = test['atom_1']\n",
    "test_atom_1_dummies = pd.get_dummies(test_atom_1_series, prefix=\"atom_1\")\n",
    "\n",
    "train_type_0_series = train['type_0']\n",
    "train_type_0_dummies = pd.get_dummies(train_type_0_series, prefix=\"type\")\n",
    "test_type_0_series = test['type_0']\n",
    "test_type_0_dummies = pd.get_dummies(test_type_0_series, prefix=\"type\")\n",
    "\n",
    "train = pd.concat([train, train_atom_0_dummies, train_atom_1_dummies, train_type_0_dummies], axis=1)\n",
    "test = pd.concat([test, test_atom_0_dummies, test_atom_1_dummies, test_type_0_dummies], axis=1)\n",
    "\n",
    "train = train.drop(['atom_0','atom_1','type_0'], axis=1)\n",
    "test = test.drop(['atom_0','atom_1', 'type_0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(train.columns)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "columns_to_normalize = [ 'x_0', 'y_0', 'z_0', 'x_1', 'y_1', 'z_1',\n",
    "       'dist', 'dist_x', 'dist_y', 'dist_z', 'dist_to_type_mean',\n",
    "       'dist_to_type_0_mean', 'dist_to_type_1_mean', 'atom_0_H', 'atom_1_C',\n",
    "       'atom_1_H', 'atom_1_N', 'type_1', 'type_2', 'type_3']\n",
    "\n",
    "train[columns_to_normalize] = min_max_scaler.fit_transform(train[columns_to_normalize])\n",
    "\n",
    "#del columns_to_normalize[0] \n",
    "test[columns_to_normalize] = min_max_scaler.fit_transform(test[columns_to_normalize])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test_internal, = train_test_split(train, test_size=0.25, random_state=42)\n",
    "\n",
    "x_train = train[columns_to_normalize]\n",
    "x_test = test_internal[columns_to_normalize]\n",
    "\n",
    "y_train = train['scalar_coupling_constant']\n",
    "y_test = test_internal['scalar_coupling_constant']\n",
    "\n",
    "x_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.isnull().values.any())\n",
    "print(x_test.isnull().values.any())\n",
    "print(y_train.isnull().values.any())\n",
    "print(y_test.isnull().values.any())\n",
    "print(y_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if(logs.get('acc')>0.9):\n",
    "      print(\"\\nReached 90% accuracy so cancelling training!\")\n",
    "      self.model.stop_training = True\n",
    "\n",
    "callbacks = myCallback()\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(20, input_shape=(20,), activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(.2),\n",
    "  tf.keras.layers.Dense(30, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(.2),\n",
    "  tf.keras.layers.Dense(50, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(.2),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(5, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(1, activation=tf.keras.activations.linear),\n",
    "])\n",
    "\n",
    "#sgd = tf.keras.optimizers.SGD(lr=0.001)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mean_absolute_error',\n",
    "              metrics=['accuracy','mean_absolute_error'])\n",
    "\n",
    "history = model.fit(x_train.values, y_train.values, validation_data=(x_test.values, y_test.values), epochs=20, callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend(loc=0)\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test[columns_to_normalize].shape\n",
    "predictions = model.predict(test[columns_to_normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.head()\n",
    "#predictions\n",
    "sub['scalar_coupling_constant'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
